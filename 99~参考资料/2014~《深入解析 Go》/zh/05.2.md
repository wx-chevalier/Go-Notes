# 5.2 goroutine 的生老病死

本小节将通过 goroutine 的创建，消亡，阻塞和恢复等过程，来观察 Go 语言的调度策略，这里就称之为生老病死吧。整个 Go 语言的调度系统是比较复杂的，为了避免结构体 M 和结构体 P 引入的其它干扰，这里主要将注意力集中到结构体 G 中，以 goroutine 为主线。

## goroutine 的创建

前面讲函数调用协议时说过 go 关键字最终被弄成了 runtime.newproc。这就是一个 goroutine 的出生，所有新的 goroutine 都是通过这个函数创建的。

runtime.newproc(size, f, args)功能就是创建一个新的 g，这个函数不能用分段栈，因为它假设参数的放置顺序是紧接着函数 f 的（见前面函数调用协议一章，有关 go 关键字调用时的内存布局）。分段栈会破坏这个布局，所以在代码中加入了标记#pragma textflag 7 表示不使用分段栈。它会调用函数 newproc1，在 newproc1 中可以使用分段栈。真正的工作是调用 newproc1 完成的。newproc1 进行下面这些动作。

首先，它会检查当前结构体 M 中的 P 中，是否有可用的结构体 G。如果有，则直接从中取一个，否则，需要分配一个新的结构体 G。如果分配了新的 G，需要将它挂到 runtime 的相关队列中。

获取了结构体 G 之后，将调用参数保存到 g 的栈，将 sp，pc 等上下文环境保存在 g 的 sched 域，这样整个 goroutine 就准备好了，整个状态和一个运行中的 goroutine 被中断时一样，只要等分配到 CPU，它就可以继续运行。

    newg->sched.sp = (uintptr)sp;
    newg->sched.pc = (byte*)runtime·goexit;
    newg->sched.g = newg;
    runtime·gostartcallfn(&newg->sched, fn);
    newg->gopc = (uintptr)callerpc;
    newg->status = Grunnable;
    newg->goid = runtime·xadd64(&runtime·sched.goidgen, 1);

然后将这个“准备好”的结构体 G 挂到当前 M 的 P 的队列中。这里会给予新的 goroutine 一次运行的机会，即：如果当前的 P 的数目没有到上限，也没有正在自旋抢 CPU 的 M，则调用 wakep 将 P 立即投入运行。

wakep 函数唤醒 P 时，调度器会试着寻找一个可用的 M 来绑定 P，必要的时候会新建 M。让我们看看新建 M 的函数 newm：

    // 新建一个m，它将以调用fn开始，或者是从调度器开始
    static void
    newm(void(*fn)(void), P *p)
    {
    	M *mp;
    	mp = runtime·allocm(p);
    	mp->nextp = p;
    	mp->mstartfn = fn;
    	runtime·newosproc(mp, (byte*)mp->g0->stackbase);
    }

runtime.newm 功能跟 newproc 相似,前者分配一个 goroutine,而后者分配一个 M。其实一个 M 就是一个操作系统线程的抽象，可以看到它会调用 runtime.newosproc。

总算看到了从 Go 的运行时库到操作系统的接口，runtime.newosproc(平台相关的)会调用系统的 runtime.clone(平台相关的)来新建一个线程，新的线程将以 runtime.mstart 为入口函数。runtime.newosproc 是个很有意思的函数，还有一些信号处理方面的细节，但是对鉴于我们是专注于调度方面，就不对它进行更细致的分析了，感兴趣的读者可以自行去 runtime/os_linux.c 看看源代码。runtime.clone 是用汇编实现的,代码在 sys_linux_amd64.s。

既然线程是以 runtime.mstart 为入口的，那么接下来看 mstart 函数。

mstart 是 runtime.newosproc 新建的系统线程的入口地址，新线程执行时会从这里开始运行。新线程的执行和 goroutine 的执行是两个概念，由于有 m 这一层对机器的抽象，是 m 在执行 g 而不是线程在执行 g。所以线程的入口是 mstart，g 的执行要到 schedule 才算入口。函数 mstart 最后调用了 schedule。

终于到了 schedule 了！

如果是从 mstart 进入到 schedule 的，那么 schedule 中逻辑非常简单，大概就这几步：

    找到一个等待运行的g
    如果g是锁定到某个M的，则让那个M运行
    否则，调用execute函数让g在当前的M中运行

execute 会恢复 newproc1 中设置的上下文，这样就跳转到新的 goroutine 去执行了。从 newproc 出生一直到运行的过程分析，到此结束!

虽然按这样 a 调用 b，b 调用 c，c 调用 d，d 调用 e 的方式去分析源代码谁看都会晕掉，但还是要重复一遍这里的读代码过程，希望感兴趣的读者可以拿着注释过的源码按顺序走一遍：

newproc -> newproc1 -> (如果 P 数目没到上限)wakep -> startm -> (可能引发)newm -> newosproc -> (线程入口)mstart -> schedule -> execute -> goroutine 运行

## 进出系统调用

假设 goroutine"生病"了，它要进入系统调用了，暂时无法继续执行。进入系统调用时，如果系统调用是阻塞的，goroutine 会被剥夺 CPU，将状态设置成 Gsyscall 后放到就绪队列。Go 的 syscall 库中提供了对系统调用的封装，它会在真正执行系统调用之前先调用函数.entersyscall，并在系统调用函数返回后调用.exitsyscall 函数。这两个函数就是通知 Go 的运行时库这个 goroutine 进入了系统调用或者完成了系统调用，调度器会做相应的调度。

比如 syscall 包中的 Open 函数，它会调用 Syscall(SYS_OPEN, uintptr(unsafe.Pointer(\_p0)), uintptr(mode), uintptr(perm))实现。这个函数是用汇编写的，在 syscall/asm_linux_amd64.s 中可以看到它的定义：

    TEXT	·Syscall(SB),7,$0
    	CALL	runtime·entersyscall(SB)
    	MOVQ	16(SP), DI
    	MOVQ	24(SP), SI
    	MOVQ	32(SP), DX
    	MOVQ	$0, R10
    	MOVQ	$0, R8
    	MOVQ	$0, R9
    	MOVQ	8(SP), AX	// syscall entry
    	SYSCALL
    	CMPQ	AX, $0xfffffffffffff001
    	JLS	ok
    	MOVQ	$-1, 40(SP)	// r1
    	MOVQ	$0, 48(SP)	// r2
    	NEGQ	AX
    	MOVQ	AX, 56(SP)  // errno
    	CALL	runtime·exitsyscall(SB)
    	RET
    ok:
    	MOVQ	AX, 40(SP)	// r1
    	MOVQ	DX, 48(SP)	// r2
    	MOVQ	$0, 56(SP)	// errno
    	CALL	runtime·exitsyscall(SB)
    	RET

可以看到它进系统调用和出系统调用时分别调用了 runtime.entersyscall 和 runtime.exitsyscall 函数。那么，这两个函数做什么特殊的处理呢？

首先，将函数的调用者的 SP,PC 等保存到结构体 G 的 sched 域中。同时，也保存到 g->gcsp 和 g->gcpc 等，这个是跟垃圾回收相关的。

然后检查结构体 Sched 中的 sysmonwait 域，如果不为 0，则将它置为 0，并调用 runtime·notewakeup(&runtime·sched.sysmonnote)。做这这一步的原因是，目前这个 goroutine 要进入 Gsyscall 状态了，它将要让出 CPU。如果有人在等待 CPU 的话，会通知并唤醒等待者，马上就有 CPU 可用了。

接下来，将 m 的 MCache 置为空，并将 m->p->m 置为空，表示进入系统调用后结构体 M 是不需要 MCache 的，并且 P 也被剥离了，将 P 的状态设置为 PSyscall。

有一个与 entersyscall 函数稍微不同的函数叫 entersyscallblock，它会告诉提示这个系统调用是会阻塞的，因此会有一点点区别。它调用的 releasep 和 handoffp。

releasep 将 P 和 M 完全分离，使 p->m 为空，m->p 也为空，剥离 m->mcache，并将 P 的状态设置为 Pidle。注意这里的区别，在非阻塞的系统调用 entersyscall 中只是设置成 Psyscall，并且也没有将 m->p 置为空。

handoffp 切换 P。将 P 从处于 syscall 或者 locked 的 M 中，切换出来交给其它 M。每个 P 中是挂了一个可执行的 G 的队列的，如果这个队列不为空，即如果 P 中还有 G 需要执行，则调用 startm 让 P 与某个 M 绑定后立刻去执行，否则将 P 挂到 idlep 队列中。

出系统调用时会调用到 runtime·exitsyscall，这个函数跟进系统调用做相反的操作。它会先检查当前 m 的 P 和它状态，如果 P 不空且状态为 Psyscall，则说明是从一个非阻塞的系统调用中返回的，这时是仍然有 CPU 可用的。因此将 p->m 设置为当前 m，将 p 的 mcache 放回到 m，恢复 g 的状态为 Grunning。否则，它是从一个阻塞的系统调用中返回的，因此之前 m 的 P 已经完全被剥离了。这时会查看调用中是否还有 idle 的 P，如果有，则将它与当前的 M 绑定。

如果从一个阻塞的系统调用中出来，并且出来的这一时刻又没有 idle 的 P 了，要怎么办呢？这种情况代码当前的 goroutine 无法继续运行了，调度器会将它的状态设置为 Grunnable，将它挂到全局的就绪 G 队列中，然后停止当前 m 并调用 schedule 函数。

## goroutine 的消亡以及状态变化

goroutine 的消亡比较简单，注意在函数 newproc1，设置了 fnstart 为 goroutine 执行的函数，而将新建的 goroutine 的 sched 域的 pc 设置为了函数 runtime.exit。当 fnstart 函数执行完返回时，它会返回到 runtime.exit 中。这时 Go 就知道这个 goroutine 要结束了，runtime.exit 中会做一些回收工作，会将 g 的状态设置为 Gdead 等，并将 g 挂到 P 的 free 队列中。

从以上的分析中，其实已经基本上经历了 goroutine 的各种状态变化。在 newproc1 中新建的 goroutine 被设置为 Grunnable 状态，投入运行时设置成 Grunning。在 entersyscall 的时候 goroutine 的状态被设置为 Gsyscall，到出系统调用时根据它是从阻塞系统调用中出来还是非阻塞系统调用中出来，又会被设置成 Grunning 或者 Grunnable 的状态。在 goroutine 最终退出的 runtime.exit 函数中，goroutine 被设置为 Gdead 状态。

等等，好像缺了什么？是的，Gidle 始终没有出现过。这个状态好像实际上没有被用到。只有一个 runtime.park 函数会使 goroutine 进入到 Gwaiting 状态，但是 park 这个有什么作用我暂时还没看懂...

goroutine 的状态变迁图：

![](https://ngte-superbed.oss-cn-beijing.aliyuncs.com/uPic/images/5.2.goroutine_state.jpg?raw=true)

## links

- [目录](preface.md)
- 上一节: [调度器相关数据结构](05.1.md)
- 下一节: [设计与演化](05.3.md)
